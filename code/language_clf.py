import random
import sys
from collections import Counter
from multiprocessing.pool import Pool

import matplotlib.colors as mcolors
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import somoclu
from sklearn.metrics import label_ranking_average_precision_score, accuracy_score
from sklearn.preprocessing import normalize, MultiLabelBinarizer
from scipy.special import softmax

from util.util import get_tokens
from util.util import stem_document

Imag_path = './images/'


def get_random_string(length=5):
    """
     Generates a random string 
    :param length: length of the string generated by the function
    :return: 
    """
    letters = 'abcdfghijlkmnopqrstuvwxyz'.strip()
    return ''.join([random.choice(letters) for i in range(length)])


def preprocess_document(document, stem_list):
    old_tokens = get_tokens(document)

    train_document = stem_document(' '.join(old_tokens), stem_list)

    new_tokens = train_document.split(' ')
    return new_tokens


class LangClf:
    stem_dic = None
    test_documents = None
    number_train_words = None
    train_documents = None
    lang_count_dic = {}
    train_recurrent_words = {}
    test_results = {}
    hits = 0

    def __init__(self, stem_dic=None, words_threshold=100):

        """
        :param stem_dic: The dictionary of stems
        :param words_threshold: The threshold for the most frequent words to be selected in the training process
        """

        if stem_dic is None:
            stem_dic = {}

        self.hits = 0
        self.train_documents = {}
        self.test_documents = {}
        self.stem_dic = stem_dic
        self.train_recurrent_words = {}
        self.test_results = {}
        self.words_threshold = words_threshold
        self.number_train_words = []
        self.labels = []
        self.softmax_list = []

    def extract_features(self, params):

        """
         Creates the profile features of training document obtaining the
         100 most frequent words.
        :param params: A tuple where the first item is the document label and second the training corpus
        :return: a tuple of (label, list of 100 most frequent words)
        """
        train_document = params[1]
        doc_id = params[0]
        tokens = preprocess_document(train_document, list(self.stem_dic[self.labels[doc_id]]))
        words_count = Counter(tokens)
        self.number_train_words.append((doc_id, sum(words_count.values())))
        return self.labels[doc_id], list(dict(words_count.most_common(self.words_threshold)).keys())

    def fit(self, X, y):
        """
        Receives the training set and count the words
        :param X: The texts list
        key:Language value: text
        :return: nothing
        """

        self.train_documents = X
        self.labels = y
        p = Pool(5)
        self.train_recurrent_words = dict(p.map(self.extract_features, list(enumerate(X))))

    def get_lang_count(self):
        return self.lang_count_dic

    def load_clf(self, train_data):
        self.train_recurrent_words = train_data

    def check_similarity(self, document):
        """
            Checks the similarity between the training dataset train_lang and the test_words data_set
        :param document:
        :return:
        """

        similarity_list = []
        for i, train_lang in enumerate(self.labels):
            # Obtain the words count in the training set
            train_words = self.train_recurrent_words[train_lang]
            _, test_recurrent_words = self.extract_features((i, document))

            # Calculate the similarity

            similarity = 0
            for w_t in train_words:
                if w_t in test_recurrent_words:
                    similarity += 1

            match = similarity / len(train_words)
            similarity_list.append(match)

        return softmax(np.array(similarity_list))

    def predict(self, document):

        """
          Predicts the language of a given language
        :param document:
        :return: return the predicted language with its similarity.
        """

        similarity = self.check_similarity(document)
        pred_index = similarity.argmax().item()

        return self.labels[pred_index]

    def _run_test(self, params):

        return params[0], self.check_similarity(params[1])

    def test(self, x_test):

        self.test_documents = x_test
        test_list = []
        p = Pool(5)

        for i, txt in enumerate(x_test):
            test_list.append((i, txt))
        self.test_results = p.map(self._run_test, test_list)
        self.test_results = sorted(self.test_results, key=lambda tup: tup[0])
        predictions = [self.labels[tup[1].argmax().item()] for tup in self.test_results]
        return predictions

    def get_test_results(self):
        return self.test_results

    def get_test_mean_size(self):
        """
        :return:The mean size of the test set in terms of number of words
        """
        sizes = []
        for text in self.test_documents.values():
            sizes.append(len(list(text)))

        return np.mean(sizes)

    def get_train_mean_size(self):
        """
        :return: The mean size of the training set in terms of number of words
        """
        sizes = []
        for text in self.train_documents:
            sizes.append(len(text))
        return np.mean(sizes)

    def get_mean_similarity(self):
        """
        :return: The mean of similarity between of the test and training set of top words
        """
        similarities = [tup[1] for tup in self.test_results.values()]
        return np.mean(similarities)

    def get_std_similarity(self):
        """
        :return: The Standard Deviation of similarity between of the test and training set of top words
        """
        similarities = [tup[1] for tup in self.test_results.values()]
        return np.std(similarities)

    def save_clf(self):
        df = pd.DataFrame(self.train_recurrent_words)
        df.to_csv('top_ranked_words.csv', index=False)

    def get_test_plot(self, title='Test Plot'):

        x = [tup[1] for tup in self.test_results.values()]
        x.sort(reverse=True)
        plt.barh(list(self.test_results.keys()), x)
        plt.title(title)
        plt.xticks(rotation=45)
        filename = title.lower() + str(self.get_mean_similarity()) + '.pdf'
        plt.savefig(Imag_path + filename, dpi=600)

    def save_results(self):
        file = open('test_results.txt', 'a')
        file.write("Threshold: " + str(self.words_threshold) + '\n')
        file.write('-' * 40)
        file.write("\nMean similarity: " + str(self.get_mean_similarity()))
        file.write("\nStandard Deviation similarity: " + str(self.get_std_similarity()))
        file.write("\nAccuracy: " + str(self.get_accuracy()))
        file.write("\nMean train size: " + str(self.get_train_mean_size()))
        file.write('-' * 40 + '\n')
        file.close()

    def _prepare_data_to_som(self, save=True):

        """
         The method creates the array of features of each language based on the
         frequency of each term
        :param save:
        :return:
        """
        labels = list(self.train_recurrent_words.keys())
        lang_word_freq_dict = {}.fromkeys(labels)

        top_tokens = [tup[0] for word_list in list(self.train_recurrent_words.values())
                      for tup in word_list]

        top_tokens = set(top_tokens)

        for label in self.train_recurrent_words.keys():

            word_freq = {}

            tokens = get_tokens(self.train_documents[label])
            train_document = stem_document(' '.join(tokens), self.stem_dic[label])
            count = Counter(train_document.split(' '))

            for word in top_tokens:
                try:
                    word_freq[word] = count[word]
                except KeyError:
                    word_freq[word] = 0

            lang_word_freq_dict[label] = word_freq
        df = pd.DataFrame.from_dict(lang_word_freq_dict).fillna(0)
        values = df.to_numpy()

        X_normalized = np.array(normalize(values, norm='l1'))
        if save:
            X_normalized = X_normalized.T
            df = pd.DataFrame(X_normalized, index=labels, columns=top_tokens, dtype='float32')
            file_name = 'profile_features-' + get_random_string() + '.csv'
            df.to_csv(file_name)
        return softmax(values)

    def run_som(self):

        """
         The method runs a unsupervised evaluation using the self-organized maps (SOM).
         It shows the differences between each language based on the features map. (see doc method prepare_data_to_som)
        :return:
        """
        X_normalized = self._prepare_data_to_som()
        labels = list(self.train_recurrent_words.keys())
        n_columns = 400
        n_rows = 400

        colors = random.choices(list(mcolors.CSS4_COLORS.keys()), k=len(labels))

        som = somoclu.Somoclu(n_columns, n_rows, data=X_normalized)
        som.train(epochs=100)

        map_filename = "map-" + get_random_string() + '.pdf'
        som.view_umatrix(bestmatches=True, bestmatchcolors=colors, labels=labels,
                         filename=map_filename)
